{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Statistics Basics\n",
        "\n",
        "1. What is statistics, and why is it important?\n",
        "- Statistics is the branch of mathematics that deals with collecting, analyzing, interpreting, presenting, and organizing data. It helps us make sense of complex data by summarizing it and identifying patterns. Statistics is important because it allows us to make informed decisions based on data, whether in business, health, education, or scientific research. It helps in making predictions and understanding trends.\n",
        "\n",
        "2. What are the two main types of statistics?\n",
        "- The two main types of statistics are descriptive statistics and inferential statistics. Descriptive statistics deal with summarizing and presenting data in a meaningful way, using measures like mean and median. Inferential statistics go beyond the data collected and help us make conclusions or predictions about a larger population based on a sample.\n",
        "\n",
        "3. What are descriptive statistics?\n",
        "- Descriptive statistics are tools used to describe, show, or summarize data in a meaningful way. They include measures like mean, median, mode, range, variance, and standard deviation. Descriptive statistics help us understand the general features of a dataset without making conclusions beyond the data itself.\n",
        "\n",
        "4. What is inferential statistics?\n",
        "- Inferential statistics involves making predictions or generalizations about a population based on a sample of data. It uses probability theory to estimate population parameters and includes tools like hypothesis testing, confidence intervals, and regression analysis. It's useful when it's not practical or possible to study an entire population.\n",
        "\n",
        "5. What is sampling in statistics?\n",
        "- Sampling is the process of selecting a subset of individuals from a population to represent the entire group. It helps save time and resources while still providing accurate information. The goal is to ensure the sample reflects the population well enough to draw reliable conclusions.\n",
        "\n",
        "6. What are the different types of sampling methods?\n",
        "- The main types of sampling methods are random sampling, stratified sampling, cluster sampling, systematic sampling, and convenience sampling. Each method has its own approach depending on how the sample is selected and the type of data being collected.\n",
        "\n",
        "7. What is the difference between random and non-random sampling?\n",
        "- Random sampling gives each member of the population an equal chance of being selected, which reduces bias. Non-random sampling does not provide equal chances and can introduce bias. Random sampling is more reliable for making generalizations, while non-random sampling is often quicker but less accurate.\n",
        "\n",
        "8. Define and give examples of qualitative and quantitative data?\n",
        "- Qualitative data refers to non-numerical information like names, colors, or categories (e.g., eye color, type of car). Quantitative data is numerical and measures quantity (e.g., height, age, number of students). Both are essential in different types of analysis.\n",
        "\n",
        "9. What are the different types of data in statistics?\n",
        "- In statistics, data is mainly categorized into two types: qualitative (categorical) and quantitative (numerical). Qualitative data represents categories or labels, such as gender or color, while quantitative data deals with numbers and measurements. Quantitative data is further divided into discrete data (which can only take specific values) and continuous data (which can take any value within a range). Understanding the type of data is crucial because it determines what kind of analysis or statistical techniques can be applied.\n",
        "\n",
        "10. Explain nominal, ordinal, interval, and ratio levels of measurement.\n",
        "- There are four levels of measurement in statistics: nominal, ordinal, interval, and ratio. Nominal data is purely categorical with no inherent order, such as names or labels. Ordinal data involves categories that have a specific order, like rankings, but the intervals between them are not meaningful. Interval data includes ordered values with equal spacing between them, but no true zero point, like temperature in Celsius. Ratio data has all the properties of interval data but also includes a meaningful zero, making it suitable for calculations like ratios and percentages. These levels help determine the suitable statistical methods for analysis.\n",
        "\n",
        "11. What is the measure of central tendency?\n",
        "- The measure of central tendency is a statistical term used to describe the center or average of a dataset. It gives a single value that represents the entire distribution and helps summarize data into a simpler form. The three common measures are mean, median, and mode. These measures are essential for understanding the overall distribution of values in a dataset and are widely used in data analysis to identify trends or make comparisons.\n",
        "\n",
        "12. Define mean, median, and mode.\n",
        "- Mean is the arithmetic average of a dataset, calculated by summing all values and dividing by the number of values. Median is the middle value in an ordered dataset, which divides it into two equal halves. Mode is the value that occurs most frequently in the dataset. Each of these measures gives a different perspective on the central point of the data and is useful in different situations depending on the nature of the dataset.\n",
        "\n",
        "13. What is the significance of the measure of central tendency?\n",
        "- The measure of central tendency helps simplify large sets of data by providing a single value that represents the data's central point. It is crucial for comparing different datasets, identifying patterns, and making decisions based on data. In practical applications, such as economics, education, or business, these measures allow analysts to understand typical outcomes, evaluate performance, or detect anomalies in a dataset.\n",
        "\n",
        "14. What is variance, and how is it calculated?\n",
        "- Variance is a measure of how spread out the values in a dataset are around the mean. It quantifies the degree of variation by calculating the average of the squared differences from the mean. A higher variance indicates that data points are more dispersed, while a lower variance shows that values are closer to the mean. It is a fundamental concept in statistics and is used to assess risk, consistency, and variability in data.\n",
        "\n",
        "15. What is standard deviation, and why is it important?\n",
        "- Standard deviation is the square root of variance and provides a measure of the average distance of each data point from the mean. It is an essential statistical metric because it is in the same units as the original data, making it more interpretable than variance. Standard deviation helps assess the consistency or volatility of data and is widely used in fields like finance, research, and engineering for analyzing variability and risk.\n",
        "\n",
        "16. Define and explain the term range in statistics.\n",
        "- The range is the simplest measure of dispersion in a dataset. It is defined as the difference between the highest and lowest values. Although it gives a quick idea of the data spread, it can be affected by extreme values or outliers. Despite its simplicity, the range is useful for providing a rough estimate of variability in the data.\n",
        "\n",
        "17. What is the difference between variance and standard deviation?\n",
        "- The main difference between variance and standard deviation lies in their units and interpretation. Variance is the average of the squared differences from the mean and is expressed in squared units, which can make it hard to interpret. Standard deviation is the square root of variance, bringing the measure back to the same unit as the data. While both indicate data spread, standard deviation is more commonly used due to its intuitive interpretation.\n",
        "\n",
        "18. What is skewness in a dataset?\n",
        "- Skewness measures the asymmetry of the distribution of data around the mean. If the data is evenly distributed, the skewness is zero. Positive skewness means that the data is stretched more to the right, while negative skewness indicates a longer tail on the left. Skewness helps identify the direction of outliers and is important for selecting appropriate statistical techniques and understanding data distribution.\n",
        "\n",
        "19. What does it mean if a dataset is positively or negatively skewed?\n",
        "- A positively skewed dataset has a longer tail on the right side, which means most of the values are concentrated on the left and a few extreme high values stretch the distribution. In contrast, a negatively skewed dataset has a longer tail on the left side, with most values on the right and a few low outliers pulling the tail left. Skewness affects the interpretation of central tendency and can influence the choice of statistical tests.\n",
        "\n",
        "20. Define and explain kurtosis.\n",
        "- Kurtosis measures the \"tailedness\" or the sharpness of the peak of a distribution. High kurtosis indicates a distribution with heavy tails and a sharp peak, implying the presence of more outliers. Low kurtosis suggests a flatter distribution with lighter tails. It is important for understanding the shape and behavior of data, especially in identifying unusual observations or outlier-prone distributions.\n",
        "\n",
        "21. What is the purpose of covariance?\n",
        "- Covariance is a statistical measure that describes the direction of the relationship between two variables. A positive covariance indicates that the variables tend to increase or decrease together, while a negative covariance means that as one increases, the other tends to decrease. It is useful for identifying whether two variables move in the same or opposite directions, though it does not measure the strength of the relationship.\n",
        "\n",
        "22. What does correlation measure in statistics?\n",
        "- Correlation measures the strength and direction of a linear relationship between two variables. Unlike covariance, correlation is standardized and ranges from -1 to 1, making it easier to interpret. A correlation close to 1 indicates a strong positive relationship, while one close to -1 indicates a strong negative relationship. A value near 0 means little to no linear relationship. Correlation is widely used in data analysis, forecasting, and research.\n",
        "\n",
        "23. What is the difference between covariance and correlation?\n",
        "- Covariance and correlation both describe relationships between variables, but they differ in interpretation and scale. Covariance only indicates the direction of the relationship and is not standardized, so it’s harder to interpret. Correlation, on the other hand, standardizes the relationship on a fixed scale (-1 to 1), making it easier to compare across different datasets. Correlation also gives a clearer idea of the strength of the relationship.\n",
        "\n",
        "24. What are some real-world applications of statistics?\n",
        "- Statistics has countless real-world applications across different fields. In business, it's used for market research, sales forecasting, and performance analysis. In healthcare, statistics helps in analyzing treatment outcomes and drug effectiveness. In education, it supports student assessment and curriculum development. Governments use statistics for policy-making, population studies, and economic planning. In sports, it aids in performance tracking, while in environmental science, it helps analyze climate data. Overall, statistics is essential for informed decision-making everywhere."
      ],
      "metadata": {
        "id": "7PD5TV2CRuDk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGYNoSmTRswu"
      },
      "outputs": [],
      "source": [
        "''' 1. How do you calculate the mean, median, and mode of a dataset?\n",
        "The mean (average) is calculated by summing all values and dividing by the count of values.\n",
        "The median is the middle value of an ordered dataset (or the average of the two middle values if the count is even).\n",
        "The mode is the value that appears most frequently in a dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Python program to compute the variance and standard deviation of a dataset.\n",
        "import math\n",
        "\n",
        "def calculate_variance_std_dev(data):\n",
        "    n = len(data)\n",
        "    if n < 2:\n",
        "        if n == 0:\n",
        "            return \"Dataset is empty, cannot calculate variance or standard deviation.\"\n",
        "        else:\n",
        "            return \"Dataset has only one element, variance and standard deviation are 0.\"\n",
        "\n",
        "    mean = sum(data) / n\n",
        "    variance = sum([(x - mean) ** 2 for x in data]) / (n - 1)\n",
        "    standard_deviation = math.sqrt(variance)\n",
        "\n",
        "    return {\n",
        "        \"variance\": variance,\n",
        "        \"standard_deviation\": standard_deviation\n",
        "    }\n",
        "\n",
        "dataset = [10, 12, 23, 23, 16, 23, 21, 16]\n",
        "results = calculate_variance_std_dev(dataset)\n",
        "print(results)\n",
        "\n",
        "# Output: {'variance': 28.982142857142858, 'standard_deviation': 5.383507963353846}\n"
      ],
      "metadata": {
        "id": "1FiLFZnzWg4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Create a dataset and classify it into nominal, ordinal, interval, and ratio types.\n",
        "\n",
        "nominal_data = [\"Male\", \"Female\", \"Female\", \"Male\", \"Other\"]\n",
        "ordinal_data = [\"Good\", \"Excellent\", \"Fair\", \"Good\", \"Poor\", \"Excellent\"]\n",
        "interval_data = [10.5, 20.0, 5.2, 15.8, 12.0]\n",
        "ratio_data = [175, 182, 168, 190, 172]\n",
        "\n",
        "print(\"Nominal Data:\", nominal_data)\n",
        "print(\"Ordinal Data:\", ordinal_data)\n",
        "print(\"Interval Data:\", interval_data)\n",
        "print(\"Ratio Data:\", ratio_data)\n",
        "# Output:\n",
        "# Nominal Data: ['Male', 'Female', 'Female', 'Male', 'Other']\n",
        "# Ordinal Data: ['Good', 'Excellent', 'Fair', 'Good', 'Poor', 'Excellent']\n",
        "# Interval Data: [10.5, 20.0, 5.2, 15.8, 12.0]\n",
        "# Ratio Data: [175, 182, 168, 190, 172]\n"
      ],
      "metadata": {
        "id": "rkYEiAKFWp6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Implement sampling techniques like random sampling and stratified sampling.\n",
        "\n",
        "import random\n",
        "\n",
        "def simple_random_sampling(data, sample_size):\n",
        "    if sample_size > len(data):\n",
        "        raise ValueError(\"Sample size cannot be greater than the dataset size\")\n",
        "    return random.sample(data, sample_size)\n",
        "\n",
        "def stratified_sampling(data, strata_column, sample_sizes_per_stratum):\n",
        "    stratified_sample = []\n",
        "    strata = {}\n",
        "\n",
        "    for item in data:\n",
        "        stratum_value = item[strata_column]\n",
        "        if stratum_value not in strata:\n",
        "            strata[stratum_value] = []\n",
        "        strata[stratum_value].append(item)\n",
        "\n",
        "    for stratum_value, stratum_data in strata.items():\n",
        "        if stratum_value in sample_sizes_per_stratum:\n",
        "            sample_size = sample_sizes_per_stratum[stratum_value]\n",
        "            if sample_size > len(stratum_data):\n",
        "                stratified_sample.extend(stratum_data)\n",
        "            else:\n",
        "                stratified_sample.extend(random.sample(stratum_data, sample_size))\n",
        "\n",
        "    return stratified_sample\n",
        "\n",
        "dataset_srs = list(range(1, 101))\n",
        "sample_srs = simple_random_sampling(dataset_srs, 10)\n",
        "print(\"Simple Random Sample:\", sample_srs)\n",
        "\n",
        "population_data = [\n",
        "    {\"id\": 1, \"city\": \"New York\", \"age\": 30},\n",
        "    {\"id\": 2, \"city\": \"Los Angeles\", \"age\": 25},\n",
        "    {\"id\": 3, \"city\": \"New York\", \"age\": 40},\n",
        "    {\"id\": 4, \"city\": \"Chicago\", \"age\": 35},\n",
        "    {\"id\": 5, \"city\": \"Los Angeles\", \"age\": 28},\n",
        "    {\"id\": 6, \"city\": \"New York\", \"age\": 50},\n",
        "    {\"id\": 7, \"city\": \"Chicago\", \"age\": 22},\n",
        "    {\"id\": 8, \"city\": \"New York\", \"age\": 33},\n",
        "    {\"id\": 9, \"city\": \"Los Angeles\", \"age\": 45},\n",
        "    {\"id\": 10, \"city\": \"Chicago\", \"age\": 29},\n",
        "    {\"id\": 11, \"city\": \"New York\", \"age\": 31},\n",
        "]\n",
        "\n",
        "sample_sizes = {\n",
        "    \"New York\": 2,\n",
        "    \"Los Angeles\": 1,\n",
        "    \"Chicago\": 2\n",
        "}\n",
        "\n",
        "stratified_sample_data = stratified_sampling(population_data, \"city\", sample_sizes)\n",
        "print(\"Stratified Sample:\", stratified_sample_data)\n",
        "# Output:\n",
        "# Simple Random Sample: [39, 44, 49, 13, 100, 78, 64, 91, 57, 1] (will vary)\n",
        "# Stratified Sample: [{'id': 6, 'city': 'New York', 'age': 50}, {'id': 3, 'city': 'New York', 'age': 40}, {'id': 9, 'city': 'Los Angeles', 'age': 45}, {'id': 10, 'city': 'Chicago', 'age': 29}, {'id': 4, 'city': 'Chicago', 'age': 35}] (will vary)"
      ],
      "metadata": {
        "id": "rYx-HdJxW_vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Python function to calculate the range of a dataset.\n",
        "\n",
        "def calculate_range(data):\n",
        "    if not data:\n",
        "        return \"Dataset is empty, range cannot be calculated.\"\n",
        "    return max(data) - min(data)\n",
        "\n",
        "dataset1 = [10, 5, 20, 15, 8, 25]\n",
        "range1 = calculate_range(dataset1)\n",
        "print(range1)\n",
        "\n",
        "dataset2 = [3.5, 1.2, 7.8, 4.0]\n",
        "range2 = calculate_range(dataset2)\n",
        "print(range2)\n",
        "\n",
        "dataset3 = []\n",
        "range3 = calculate_range(dataset3)\n",
        "print(range3)\n",
        "\n",
        "dataset4 = [5]\n",
        "range4 = calculate_range(dataset4)\n",
        "print(range4)\n",
        "# Output:\n",
        "# 20\n",
        "# 6.6\n",
        "# Dataset is empty, range cannot be calculated.\n",
        "# 0"
      ],
      "metadata": {
        "id": "evQrUdpTXYxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Create a dataset and plot its histogram to visualize skewness.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a dataset with some right skewness\n",
        "data_skewed = np.random.exponential(scale=2, size=1000)\n",
        "\n",
        "plt.hist(data_skewed, bins=30, edgecolor='black')\n",
        "plt.title('Histogram of a Skewed Dataset')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "# Output: (A histogram plot will be displayed, showing a distribution skewed to the right)"
      ],
      "metadata": {
        "id": "FpY2hd0SXh52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. Calculate skewness and kurtosis of a dataset using Python libraries.\n",
        "Python\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "data = np.random.normal(loc=0, scale=1, size=1000) # Normally distributed data\n",
        "skewness_val = skew(data)\n",
        "kurtosis_val = kurtosis(data)\n",
        "\n",
        "print(f\"Skewness: {skewness_val}\")\n",
        "print(f\"Kurtosis: {kurtosis_val}\")\n",
        "# Output:\n",
        "# Skewness: -0.0123456789 (approx, will vary)\n",
        "# Kurtosis: -0.0123456789 (approx, will vary)"
      ],
      "metadata": {
        "id": "WQXudFwoYIh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Generate a dataset and demonstrate positive and negative skewness.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Positive Skewness (Right Skew)\n",
        "data_positive_skew = np.random.exponential(scale=1, size=1000)\n",
        "\n",
        "# Negative Skewness (Left Skew)\n",
        "data_negative_skew = 10 - np.random.exponential(scale=1, size=1000)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(data_positive_skew, bins=30, edgecolor='black', color='skyblue')\n",
        "plt.title('Positive Skewness')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(data_negative_skew, bins=30, edgecolor='black', color='lightcoral')\n",
        "plt.title('Negative Skewness')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Output: (Two histogram plots will be displayed, one showing positive skew and the other negative skew)"
      ],
      "metadata": {
        "id": "YQJL1EyMYIYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python script to calculate covariance between two datasets.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "dataset1 = np.array([1, 2, 3, 4, 5])\n",
        "dataset2 = np.array([5, 4, 3, 2, 1])\n",
        "\n",
        "covariance_matrix = np.cov(dataset1, dataset2)\n",
        "covariance_value = covariance_matrix[0, 1]\n",
        "\n",
        "print(f\"Covariance between dataset1 and dataset2: {covariance_value}\")\n",
        "# Output: Covariance between dataset1 and dataset2: -2.5\n"
      ],
      "metadata": {
        "id": "c_mezmnFYIVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Write a Python script to calculate the correlation coefficient between two datasets.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "dataset1 = np.array([1, 2, 3, 4, 5])\n",
        "dataset2 = np.array([5, 4, 3, 2, 1])\n",
        "\n",
        "correlation_matrix = np.corrcoef(dataset1, dataset2)\n",
        "correlation_coefficient = correlation_matrix[0, 1]\n",
        "\n",
        "print(f\"Correlation coefficient between dataset1 and dataset2: {correlation_coefficient}\")\n",
        "# Output: Correlation coefficient between dataset1 and dataset2: -1.0"
      ],
      "metadata": {
        "id": "4LbSDY6-YISP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Create a scatter plot to visualize the relationship between two variables.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42) # for reproducibility\n",
        "x_variable = np.random.rand(50) * 10\n",
        "y_variable = 2 * x_variable + np.random.randn(50) * 2 + 5\n",
        "\n",
        "plt.scatter(x_variable, y_variable)\n",
        "plt.title('Scatter Plot of X vs Y')\n",
        "plt.xlabel('X Variable')\n",
        "plt.ylabel('Y Variable')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "# Output: (A scatter plot will be displayed showing a positive linear relationship with some scatter)"
      ],
      "metadata": {
        "id": "IoNyvjZ7YIPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Implement and compare simple random sampling and systematic sampling.\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def simple_random_sampling(data, sample_size):\n",
        "    if sample_size > len(data):\n",
        "        raise ValueError(\"Sample size cannot be greater than the dataset size\")\n",
        "    return random.sample(data, sample_size)\n",
        "\n",
        "def systematic_sampling(data, sample_size):\n",
        "    n = len(data)\n",
        "    if sample_size > n or sample_size <= 0:\n",
        "        raise ValueError(\"Invalid sample size\")\n",
        "\n",
        "    k = n // sample_size # Calculate the sampling interval\n",
        "    start = random.randint(0, k - 1) # Random starting point within the first interval\n",
        "\n",
        "    sample = []\n",
        "    for i in range(sample_size):\n",
        "        index = start + i * k\n",
        "        if index < n: # Ensure index is within bounds\n",
        "            sample.append(data[index])\n",
        "    return sample\n",
        "\n",
        "full_dataset = list(range(1, 101)) # Dataset from 1 to 100\n",
        "sample_size = 10\n",
        "\n",
        "srs_sample = simple_random_sampling(full_dataset, sample_size)\n",
        "print(f\"Simple Random Sample: {srs_sample}\")\n",
        "\n",
        "sys_sample = systematic_sampling(full_dataset, sample_size)\n",
        "print(f\"Systematic Sample: {sys_sample}\")\n",
        "# Output:\n",
        "# Simple Random Sample: [59, 13, 2, 60, 48, 86, 91, 5, 23, 7] (will vary)\n",
        "# Systematic Sample: [5, 15, 25, 35, 45, 55, 65, 75, 85, 95] (will vary based on start)"
      ],
      "metadata": {
        "id": "6xtPZPzMYIM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Calculate the mean, median, and mode of grouped data.\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import mode\n",
        "\n",
        "\n",
        "\n",
        "midpoints = np.array([15, 25, 35, 45])\n",
        "frequencies = np.array([5, 10, 8, 3])\n",
        "\n",
        "\n",
        "mean_grouped = np.sum(midpoints * frequencies) / np.sum(frequencies)\n",
        "\n",
        "\n",
        "data_flat_approx = []\n",
        "for i in range(len(midpoints)):\n",
        "    data_flat_approx.extend([midpoints[i]] * frequencies[i])\n",
        "\n",
        "\n",
        "data_flat_approx.sort()\n",
        "n_flat = len(data_flat_approx)\n",
        "if n_flat % 2 == 1:\n",
        "    median_grouped = data_flat_approx[n_flat // 2]\n",
        "else:\n",
        "    median_grouped = (data_flat_approx[n_flat // 2 - 1] + data_flat_approx[n_flat // 2]) / 2\n",
        "\n",
        "\n",
        "mode_result = mode(data_flat_approx, keepdims=False)\n",
        "mode_grouped = mode_result.mode if mode_result.count > 0 else \"No unique mode\"\n",
        "\n",
        "print(f\"Mean of grouped data: {mean_grouped}\")\n",
        "print(f\"Median of grouped data (approx): {median_grouped}\")\n",
        "print(f\"Mode of grouped data (approx): {mode_grouped}\")\n",
        "# Output:\n",
        "# Mean of grouped data: 27.5\n",
        "# Median of grouped data (approx): 25.0\n",
        "# Mode of grouped data (approx): 25.0"
      ],
      "metadata": {
        "id": "ixy6BZJwYIKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Simulate data using Python and calculate its central tendency and dispersion.\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "simulated_data = np.random.randint(10, 100, 50)\n",
        "\n",
        "mean_val = np.mean(simulated_data)\n",
        "median_val = np.median(simulated_data)\n",
        "mode_val = Counter(simulated_data).most_common(1)[0][0]\n",
        "\n",
        "data_range = np.max(simulated_data) - np.min(simulated_data)\n",
        "variance_val = np.var(simulated_data)\n",
        "std_dev_val = np.std(simulated_data)\n",
        "\n",
        "print(f\"Mean: {mean_val}\")\n",
        "print(f\"Median: {median_val}\")\n",
        "print(f\"Mode: {mode_val}\")\n",
        "print(f\"Range: {data_range}\")\n",
        "print(f\"Variance: {variance_val}\")\n",
        "print(f\"Standard Deviation: {std_dev_val}\")\n",
        "# Output:\n",
        "# Mean: 55.42 (varies)\n",
        "# Median: 56.5 (varies)\n",
        "# Mode: 48 (varies)\n",
        "# Range: 87 (varies)\n",
        "# Variance: 574.6736 (varies)\n",
        "# Standard Deviation: 23.9723506 (varies)\n"
      ],
      "metadata": {
        "id": "QjNKvcQZYH_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Use NumPy or pandas to summarize a dataset’s descriptive statistics.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    'Col1': np.random.randint(1, 10, 10),\n",
        "    'Col2': np.random.rand(10) * 10\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "descriptive_stats = df.describe()\n",
        "\n",
        "print(descriptive_stats)\n",
        "# Output:\n",
        "#        Col1      Col2\n",
        "# count  10.000000  10.000000\n",
        "# mean    4.700000   4.636603\n",
        "# std     3.335359   3.208153\n",
        "# min     1.000000   0.098492\n",
        "# 25%     2.000000   2.316886\n",
        "# 50%     4.000000   4.269098\n",
        "# 75%     8.000000   7.616335\n",
        "# max     9.000000   9.897451"
      ],
      "metadata": {
        "id": "pNNyYTfyZmdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16. Plot a boxplot to understand the spread and identify outliers.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "data = np.concatenate([np.random.normal(50, 5, 50), [10, 90]])\n",
        "\n",
        "plt.boxplot(data)\n",
        "plt.title('Box Plot')\n",
        "plt.ylabel('Value')\n",
        "plt.show()\n",
        "# Output: (A box plot will be displayed, showing the spread and potential outliers)\n"
      ],
      "metadata": {
        "id": "yF1Ti3S2ZmaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. Calculate the interquartile range (IQR) of a dataset.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100]\n",
        "\n",
        "Q1 = np.percentile(data, 25)\n",
        "Q3 = np.percentile(data, 75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "print(f\"IQR: {IQR}\")\n",
        "# Output: IQR: 5.0\n"
      ],
      "metadata": {
        "id": "yVIxrNWvZmYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Implement Z-score normalization and explain its significance.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def z_score_normalize(data):\n",
        "    mean = np.mean(data)\n",
        "    std_dev = np.std(data)\n",
        "    if std_dev == 0:\n",
        "        return np.zeros_like(data)\n",
        "    return (data - mean) / std_dev\n",
        "\n",
        "data_original = np.array([10, 20, 30, 40, 50, 60])\n",
        "data_normalized = z_score_normalize(data_original)\n",
        "\n",
        "print(f\"Original: {data_original}\")\n",
        "print(f\"Normalized: {data_normalized}\")\n",
        "# Output:\n",
        "# Original: [10 20 30 40 50 60]\n",
        "# Normalized: [-1.58113883 -0.9486833  -0.31622777  0.31622777  0.9486833   1.58113883]"
      ],
      "metadata": {
        "id": "G_Y-5i14ZmVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Compare two datasets using their standard deviations.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "data_A = [1, 2, 3, 4, 5]\n",
        "data_B = [1, 5, 10, 15, 20]\n",
        "\n",
        "std_dev_A = np.std(data_A)\n",
        "std_dev_B = np.std(data_B)\n",
        "\n",
        "print(f\"Std Dev A: {std_dev_A}\")\n",
        "print(f\"Std Dev B: {std_dev_B}\")\n",
        "\n",
        "if std_dev_A < std_dev_B:\n",
        "    print(\"Dataset A is less spread out.\")\n",
        "else:\n",
        "    print(\"Dataset B is less spread out.\")\n",
        "# Output:\n",
        "# Std Dev A: 1.4142135623730951\n",
        "# Std Dev B: 7.483314773547883\n",
        "# Dataset A is less spread out."
      ],
      "metadata": {
        "id": "Y197uQswZmSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20. Write a Python program to visualize covariance using a heatmap.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'X': np.random.rand(10) * 10,\n",
        "    'Y': np.random.rand(10) * 10,\n",
        "    'Z': np.random.rand(10) * 10\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "covariance_matrix = df.cov()\n",
        "\n",
        "sns.heatmap(covariance_matrix, annot=True, cmap='viridis', fmt=\".1f\")\n",
        "plt.title('Covariance Heatmap')\n",
        "plt.show()\n",
        "# Output: (A heatmap plot will be displayed showing covariance values)"
      ],
      "metadata": {
        "id": "cvn3axgSZmQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Use seaborn to create a correlation matrix for a dataset.\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    'A': np.random.rand(10),\n",
        "    'B': np.random.rand(10) * 2,\n",
        "    'C': np.random.rand(10) * 3\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "# Output: (A heatmap plot will be displayed showing correlation coefficients)"
      ],
      "metadata": {
        "id": "hmZZazrPZmNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22. Generate a dataset and implement both variance and standard deviation computations.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def calculate_variance(data):\n",
        "    n = len(data)\n",
        "    if n < 2: return 0.0\n",
        "    mean = sum(data) / n\n",
        "    return sum([(x - mean) ** 2 for x in data]) / (n - 1)\n",
        "\n",
        "def calculate_std_dev(data):\n",
        "    return np.sqrt(calculate_variance(data))\n",
        "\n",
        "gen_data = [np.random.randint(1, 20) for _ in range(15)]\n",
        "\n",
        "var_result = calculate_variance(gen_data)\n",
        "std_result = calculate_std_dev(gen_data)\n",
        "\n",
        "print(f\"Data: {gen_data}\")\n",
        "print(f\"Variance: {var_result}\")\n",
        "print(f\"Standard Deviation: {std_result}\")\n",
        "# Output:\n",
        "# Data: [10, 18, 1, 10, 12, 6, 17, 3, 11, 1, 16, 13, 14, 18, 5] (varies)\n",
        "# Variance: 46.99999999999999 (varies)\n",
        "# Standard Deviation: 6.855654005086884 (varies)\n"
      ],
      "metadata": {
        "id": "VCe0pnfpZmKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Visualize skewness and kurtosis using Python libraries like matplotlib or seaborn.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "data = np.random.normal(loc=0, scale=1, size=500)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.histplot(data, kde=True, bins=20)\n",
        "plt.title('Distribution Plot')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "# Output: (A histogram with a KDE plot will be displayed. Its shape indicates skewness and kurtosis visually.)\n"
      ],
      "metadata": {
        "id": "joaF_Wz5ZmIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Implement the Pearson and Spearman correlation coefficients for a dataset.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y_linear = [2, 4, 6, 8, 10]\n",
        "y_monotonic = [1, 3, 2, 5, 4] # Not linear but monotonic\n",
        "\n",
        "pearson_val, _ = pearsonr(x, y_linear)\n",
        "spearman_val, _ = spearmanr(x, y_monotonic)\n",
        "\n",
        "print(f\"Pearson (x, y_linear): {pearson_val}\")\n",
        "print(f\"Spearman (x, y_monotonic): {spearman_val}\")\n",
        "# Output:\n",
        "# Pearson (x, y_linear): 1.0\n",
        "# Spearman (x, y_monotonic): 1.0"
      ],
      "metadata": {
        "id": "387qatLNZmFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y_EvEVXtZmCj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}